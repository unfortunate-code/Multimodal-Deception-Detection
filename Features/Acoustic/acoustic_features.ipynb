{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from transformers import HubertForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTHFUL_PATH = '../../data/Acoustic/Truthful/'\n",
    "DECEPTIVE_PATH = '../../data/Acoustic/Deceptive/'\n",
    "TRUTHFUL_FILES = os.listdir(TRUTHFUL_PATH)\n",
    "DECEPTIVE_FILES = os.listdir(DECEPTIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [20:34<00:00, 20.57s/it]   \n",
      "100%|██████████| 61/61 [17:39<00:00, 17.38s/it] \n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(model, processor):\n",
    "    embeddings_map = {}\n",
    "    for truthful_file in tqdm(TRUTHFUL_FILES):\n",
    "        speech, rate = librosa.load(TRUTHFUL_PATH + truthful_file, sr=16000)\n",
    "        input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values\n",
    "        features = model(input_values, output_hidden_states=True).hidden_states[-1].squeeze(0).mean(0).detach().numpy()\n",
    "        embeddings_map[truthful_file] = features\n",
    "        gc.collect()\n",
    "    for deceptive_file in tqdm(DECEPTIVE_FILES):\n",
    "        speech, rate = librosa.load(DECEPTIVE_PATH + deceptive_file, sr=16000)\n",
    "        input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values\n",
    "        features = model(input_values, output_hidden_states=True).hidden_states[-1].squeeze(0).mean(0).detach().numpy()\n",
    "        embeddings_map[deceptive_file] = features\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")  \n",
    "embeddings_map = get_embeddings(model, processor)\n",
    "with open('../../embeddings/acoustic_features_1.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = '../../embeddings/AudioEmbeddings/'\n",
    "EMBEDDINGS_FILES = os.listdir(EMBEDDINGS_PATH)\n",
    "embeddings_map = {}\n",
    "for file in EMBEDDINGS_FILES:\n",
    "    with open(EMBEDDINGS_PATH + file, 'rb') as f:\n",
    "        embeddings_map[file.split('.')[0] + '.wav'] = pickle.load(f).squeeze().mean(0).detach().numpy()\n",
    "with open('../../embeddings/acoustic_features_2.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 213/213 [00:00<00:00, 19.4kB/s]\n",
      "c:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alame\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.66k/1.66k [00:00<00:00, 142kB/s]\n",
      "c:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:53: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'superb/hubert-base-superb-er'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'superb/hubert-base-superb-er' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:51\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\processing_utils.py:184\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mInstantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m        [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\processing_utils.py:228\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 228\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:700\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 700\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_py\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    701\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'superb/hubert-base-superb-er'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'superb/hubert-base-superb-er' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processor \u001b[39m=\u001b[39m Wav2Vec2Processor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39msuperb/hubert-base-superb-er\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m HubertForCTC\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39msuperb/hubert-base-superb-er\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[0;32m      3\u001b[0m embeddings_map \u001b[39m=\u001b[39m get_embeddings(model, processor)\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:63\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m     54\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading a tokenizer inside \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from a config that does not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m feature_extractor \u001b[39m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 63\u001b[0m tokenizer \u001b[39m=\u001b[39m Wav2Vec2CTCTokenizer\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(feature_extractor\u001b[39m=\u001b[39mfeature_extractor, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mc:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1783\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1784\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1785\u001b[0m     )\n\u001b[0;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1796\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'superb/hubert-base-superb-er'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'superb/hubert-base-superb-er' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"superb/hubert-base-superb-er\")\n",
    "model = HubertForCTC.from_pretrained(\"superb/hubert-base-superb-er\")  \n",
    "embeddings_map = get_embeddings(model, processor)\n",
    "with open('../../embeddings/acoustic_features_3.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 64)\n",
    "        self.linear2 = torch.nn.Linear(64, 2)\n",
    "        # Xavier initialize the linear layer\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(torch.nn.functional.relu(x))\n",
    "        return self.linear2(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifierPCA(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(AudioClassifierPCA, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 16)\n",
    "        self.linear2 = torch.nn.Linear(16, 2)\n",
    "        # Xavier initialize the linear layer\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(torch.nn.functional.relu(x))\n",
    "        return self.linear2(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, embeddings_file):\n",
    "        with open(embeddings_file, 'rb') as f:\n",
    "            embeddings_map = pickle.load(f)\n",
    "        self.data = [embeddings_map[file] for file in data]\n",
    "        self.labels = [0 if file.split('_')[1] == 'truth' else 1 for file in data]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetPCA(Dataset):\n",
    "    def __init__(self, data, embeddings_file, pca, sc, test=False):\n",
    "        with open(embeddings_file, 'rb') as f:\n",
    "            embeddings_map = pickle.load(f)\n",
    "        self.data = np.array([embeddings_map[file] for file in data])\n",
    "        if not test:\n",
    "            self.data = sc.fit_transform(self.data)\n",
    "            self.data = pca.fit_transform(self.data)\n",
    "        else:\n",
    "            self.data = sc.transform(self.data)\n",
    "            self.data = pca.transform(self.data)\n",
    "        self.data = torch.from_numpy(self.data).float()\n",
    "        self.labels = [0 if file.split('_')[1] == 'truth' else 1 for file in data]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(embeddings_map_file, train_batch_size, do_pca, embedding_size):\n",
    "    truthful_data = list(TRUTHFUL_FILES)\n",
    "    deceptive_data = list(DECEPTIVE_FILES)\n",
    "    for i in range(10):\n",
    "        val_data = truthful_data[i*6:(i+1)*6] + deceptive_data[i*6:(i+1)*6]\n",
    "        train_data = truthful_data[:i*6] + truthful_data[(i+1)*6:] + deceptive_data[:i*6] + deceptive_data[(i+1)*6:]\n",
    "        if not do_pca:\n",
    "            train_dataset = AudioDataset(train_data, embeddings_map_file)\n",
    "            val_dataset = AudioDataset(val_data, embeddings_map_file)\n",
    "        else:\n",
    "            sc = StandardScaler()\n",
    "            pca = PCA(n_components=embedding_size)\n",
    "            train_dataset = AudioDatasetPCA(train_data, embeddings_map_file, pca, sc)\n",
    "            val_dataset = AudioDatasetPCA(val_data, embeddings_map_file, pca, sc, test=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=12)\n",
    "        yield train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_fold(train_loader, val_loader, num_epochs, embedding_size, do_pca):\n",
    "    if not do_pca:\n",
    "        model = AudioClassifier(embedding_size)\n",
    "    else:\n",
    "        model = AudioClassifierPCA(embedding_size)\n",
    "    model.to(device)\n",
    "    learning_rate = 1e-3\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "    return eval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(embeddings_map_file, train_batch_size, num_epochs, embedding_size = 1024, do_pca = False):\n",
    "    accuracies = []\n",
    "    for train_loader, val_loader in kfold(embeddings_map_file, train_batch_size, do_pca, embedding_size):\n",
    "        accuracies.append(run_one_fold(train_loader, val_loader, num_epochs, embedding_size, do_pca))\n",
    "    return sum(accuracies) / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for hubert-large-ls960-ft \n",
    "get_model_accuracy('../../embeddings/acoustic_features_1.pkl', 4, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for hubert-large-ls960-ft with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_1.pkl', 2, 40, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for hubert-large-ls960-ft with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_1.pkl', 2, 30, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for hubert-large-ls960-ft with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_1.pkl', 4, 30, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for hubert-large-ls960-ft with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_1.pkl', 4, 40, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for vggish\n",
    "get_model_accuracy('../../embeddings/acoustic_features_2.pkl', 4, 30, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for vggish with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_2.pkl', 4, 30, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for vggish\n",
    "get_model_accuracy('../../embeddings/acoustic_features_2.pkl', 4, 40, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5916666666666667"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model accuracy for vggish with pca\n",
    "get_model_accuracy('../../embeddings/acoustic_features_2.pkl', 4, 40, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
