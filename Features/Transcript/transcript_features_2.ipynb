{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.functional as F\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DistilBertModel, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deceptive size: 75\n",
      "Truthful size: 74\n"
     ]
    }
   ],
   "source": [
    "DECEPTIVE_DIR = '../../Dataset2/Text/Lie/'\n",
    "deceptive = os.listdir(DECEPTIVE_DIR)\n",
    "TRUTHFUL_DIR = '../../Dataset2/Text/Truth/'\n",
    "truthful = os.listdir(TRUTHFUL_DIR)\n",
    "print('Deceptive size:', len(deceptive))\n",
    "print('Truthful size:', len(truthful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(115, '33-47-982.txt'), (117, '09-18-253.txt'), (124, '22-05-764.txt'), (126, '57-43-236.txt'), (132, '18-42-263.txt'), (143, '38-26-565.txt'), (149, '05-50-814.txt'), (151, '04-30-467.txt'), (162, '15-13-468.txt'), (166, '31-08-51.txt'), (168, '11-17-124.txt'), (180, '17-10-481.txt'), (181, '14-45-849.txt'), (183, '08-33-112.txt'), (190, '27-35-915.txt'), (193, '00-34-288.txt'), (193, '06-18-285.txt'), (194, '02-20-877.txt'), (194, '12-03-237.txt'), (198, '11-22-778.txt'), (200, '35-42-851.txt'), (201, '00-13-953.txt'), (203, '31-59-194.txt'), (208, '09-02-798.txt'), (211, '18-39-581.txt'), (214, '13-27-771.txt'), (216, '37-36-401.txt'), (219, '12-05-243.txt'), (223, '36-53-693.txt'), (227, '50-29-214.txt'), (228, '15-00-532.txt'), (229, '18-48-414.txt'), (230, '06-57-279.txt'), (234, '21-03-94.txt'), (236, '48-39-430.txt'), (239, '29-48-550.txt'), (241, '12-23-235.txt'), (242, '06-44-152.txt'), (244, '27-19-148.txt'), (244, '57-52-839.txt'), (246, '17-15-163.txt'), (249, '09-14-629.txt'), (249, '44-02-309.txt'), (254, '40-18-142.txt'), (255, '04-38-645.txt'), (256, '20-31-480.txt'), (257, '14-08-617.txt'), (261, '22-31-901.txt'), (262, '04-01-129.txt'), (263, '11-26-504.txt'), (263, '30-42-250.txt'), (267, '28-32-985.txt'), (271, '22-41-332.txt'), (272, '09-44-142.txt'), (275, '11-59-155.txt'), (275, '56-40-959.txt'), (277, '20-05-755.txt'), (278, '57-19-201.txt'), (280, '44-30-30.txt'), (281, '27-51-501.txt'), (283, '02-40-950.txt'), (283, '16-56-369.txt'), (289, '15-29-238.txt'), (291, '19-35-145.txt'), (291, '27-37-757.txt'), (291, '59-09-521.txt'), (292, '30-46-620.txt'), (294, '01-50-492.txt'), (295, '03-47-890.txt'), (295, '10-48-352.txt'), (296, '02-44-269.txt'), (298, '22-10-468.txt'), (298, '46-30-695.txt'), (301, '18-12-410.txt'), (305, '19-04-926.txt'), (306, '39-18-93.txt'), (309, '58-57-956.txt'), (312, '05-15-634.txt'), (317, '12-38-74.txt'), (317, '14-04-955.txt'), (317, '24-30-926.txt'), (322, '04-57-574.txt'), (324, '03-42-701.txt'), (327, '16-40-42.txt'), (331, '13-11-507.txt'), (331, '30-40-756.txt'), (334, '21-24-649.txt'), (340, '11-22-452.txt'), (340, '26-17-966.txt'), (342, '14-08-252.txt'), (345, '06-50-611.txt'), (345, '18-43-166.txt'), (346, '19-27-616.txt'), (350, '02-28-29.txt'), (354, '13-20-248.txt'), (355, '51-16-907.txt'), (356, '03-12-820.txt'), (356, '05-49-922.txt'), (357, '48-00-366.txt'), (358, '11-09-515.txt'), (358, '11-44-333.txt'), (358, '33-07-644.txt'), (361, '21-37-95.txt'), (364, '20-40-167.txt'), (366, '05-44-437.txt'), (367, '38-55-777.txt'), (367, '41-41-762.txt'), (368, '22-35-549.txt'), (373, '28-52-283.txt'), (378, '26-15-622.txt'), (380, '29-09-99.txt'), (381, '36-37-921.txt'), (390, '01-38-934.txt'), (400, '20-28-731.txt'), (402, '31-23-340.txt'), (406, '02-09-31.txt'), (408, '16-48-797.txt'), (410, '16-14-317.txt'), (410, '28-32-854.txt'), (410, '47-20-540.txt'), (423, '37-32-755.txt'), (424, '30-57-338.txt'), (425, '24-24-724.txt'), (425, '24-43-478.txt'), (429, '40-07-485.txt'), (433, '15-58-679.txt'), (447, '01-01-49.txt'), (447, '16-19-112.txt'), (453, '28-45-33.txt'), (454, '39-49-946.txt'), (456, '15-21-96.txt'), (457, '07-53-382.txt'), (468, '09-32-917.txt'), (476, '38-29-906.txt'), (477, '12-24-294.txt'), (482, '50-50-13.txt'), (484, '12-07-438.txt'), (486, '48-02-966.txt'), (498, '36-04-870.txt'), (506, '20-38-706.txt'), (538, '19-05-68.txt'), (557, '01-25-367.txt'), (564, '11-45-95.txt'), (565, '03-23-90.txt'), (579, '54-10-129.txt'), (580, '58-28-695.txt'), (606, '13-01-549.txt'), (614, '21-16-706.txt'), (617, '05-06-501.txt')]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "for file in deceptive:\n",
    "    with open(DECEPTIVE_DIR + file, 'r') as f:\n",
    "        text = f.read()\n",
    "        l = len(text.split())\n",
    "        A.append((l, file))\n",
    "for file in truthful:\n",
    "    with open(TRUTHFUL_DIR + file, 'r') as f:\n",
    "        text = f.read()\n",
    "        l = len(text.split())\n",
    "        A.append((l, file))\n",
    "print(sorted(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        if type(model) is DistilBertModel:\n",
    "            return outputs[0][0, 0, :]\n",
    "        else:\n",
    "            hidden_states = outputs[2]\n",
    "            token_embeddings = (hidden_states[-1][0, 0, :] + hidden_states[-2][0, 0, :] + hidden_states[-3][0, 0, :] + hidden_states[-4][0, 0, :]) / 4\n",
    "            return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(bert, tokenizer, output_file):\n",
    "    embedding_map = {}\n",
    "    for file in truthful:\n",
    "        with open(TRUTHFUL_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            splits = text.split()\n",
    "            subtexts = []\n",
    "            i = 0\n",
    "            while i < len(splits):\n",
    "                subtexts.append(' '.join(splits[i:i+350]))\n",
    "                # Allow a 20% overlap between subtexts\n",
    "                i += 280\n",
    "            embeddings = []\n",
    "            for subtext in subtexts:\n",
    "                tokenized_text, tokens_tensor, segments_tensor = prepare_text(subtext, tokenizer)\n",
    "                embeddings.append(get_bert_embeddings(tokens_tensor, segments_tensor, bert))\n",
    "            embedding_map[file.split('.')[0]] = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    for file in deceptive:\n",
    "        with open(DECEPTIVE_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            splits = text.split()\n",
    "            subtexts = []\n",
    "            i = 0\n",
    "            while i < len(splits):\n",
    "                subtexts.append(' '.join(splits[i:i+350]))\n",
    "                # Allow a 20% overlap between subtexts\n",
    "                i += 280\n",
    "            embeddings = []\n",
    "            for subtext in subtexts:\n",
    "                tokenized_text, tokens_tensor, segments_tensor = prepare_text(subtext, tokenizer)\n",
    "                embeddings.append(get_bert_embeddings(tokens_tensor, segments_tensor, bert))\n",
    "            embedding_map[file.split('.')[0]] = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(embedding_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "bert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "save_embeddings(bert, tokenizer, '../../embeddings2/transcript_features_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emotion = AutoModelForSequenceClassification.from_pretrained(\"ncduy/bert-base-cased-finetuned-emotion\", output_hidden_states = True).bert\n",
    "bert_emotion.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncduy/bert-base-cased-finetuned-emotion\")\n",
    "save_embeddings(bert_emotion, tokenizer, '../../embeddings2/transcript_features_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transformersbook/distilbert-base-uncased-finetuned-emotion were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "distil_bert_emotion = AutoModel.from_pretrained(\"transformersbook/distilbert-base-uncased-finetuned-emotion\", output_hidden_states = True)\n",
    "distil_bert_emotion.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"transformersbook/distilbert-base-uncased-finetuned-emotion\")\n",
    "save_embeddings(distil_bert_emotion, tokenizer, '../../embeddings2/transcript_features_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "distil_bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "distil_bert_model.eval()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "save_embeddings(distil_bert_model, tokenizer, '../../embeddings2/transcript_features_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentence_transformer_embeddings(model, output_file, max_length):\n",
    "    embedding_map = {}\n",
    "    for file in truthful:\n",
    "        with open(TRUTHFUL_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            splits = text.split()\n",
    "            subtexts = []\n",
    "            i = 0\n",
    "            while i < len(splits):\n",
    "                subtexts.append(' '.join(splits[i:i+max_length]))\n",
    "                # Allow a 20% overlap between subtexts\n",
    "                i += int(max_length*0.8)\n",
    "            embeddings = []\n",
    "            for subtext in subtexts:\n",
    "                embeddings.append(model.encode(subtext))\n",
    "            embedding_map[file.split('.')[0]] = np.mean(embeddings, axis=0)\n",
    "    for file in deceptive:\n",
    "        with open(DECEPTIVE_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            splits = text.split()\n",
    "            subtexts = []\n",
    "            i = 0\n",
    "            while i < len(splits):\n",
    "                subtexts.append(' '.join(splits[i:i+max_length]))\n",
    "                # Allow a 20% overlap between subtexts\n",
    "                i += int(max_length*0.8)\n",
    "            embeddings = []\n",
    "            for subtext in subtexts:\n",
    "                embeddings.append(model.encode(subtext))\n",
    "            embedding_map[file.split('.')[0]] = np.mean(embeddings, axis=0)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(embedding_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniLM_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "save_sentence_transformer_embeddings(miniLM_model, '../../embeddings2/transcript_features_5.pkl', 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "save_sentence_transformer_embeddings(mpnet_model, '../../embeddings2/transcript_features_6.pkl', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "bert = BertModel.from_pretrained('bert-large-uncased', output_hidden_states = True)\n",
    "bert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "save_embeddings(bert, tokenizer, '../../embeddings2/transcript_features_7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
