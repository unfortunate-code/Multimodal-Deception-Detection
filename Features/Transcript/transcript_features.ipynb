{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.functional as F\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DistilBertModel, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deceptive size: 61\n",
      "Truthful size: 60\n"
     ]
    }
   ],
   "source": [
    "DECEPTIVE_DIR = '../../data/Transcription/Deceptive/'\n",
    "deceptive = os.listdir(DECEPTIVE_DIR)\n",
    "TRUTHFUL_DIR = '../../data/Transcription/Truthful/'\n",
    "truthful = os.listdir(TRUTHFUL_DIR)\n",
    "print('Deceptive size:', len(deceptive))\n",
    "print('Truthful size:', len(truthful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        if type(model) is DistilBertModel:\n",
    "            return outputs[0][0, 0, :]\n",
    "        else:\n",
    "            hidden_states = outputs[2]\n",
    "            token_embeddings = (hidden_states[-1][0, 0, :] + hidden_states[-2][0, 0, :] + hidden_states[-3][0, 0, :] + hidden_states[-4][0, 0, :]) / 4\n",
    "            return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(bert, tokenizer, output_file):\n",
    "    embedding_map = {}\n",
    "    for file in truthful:\n",
    "        with open(TRUTHFUL_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            tokenized_text, tokens_tensor, segments_tensor = prepare_text(text, tokenizer)\n",
    "            embeddings = get_bert_embeddings(tokens_tensor, segments_tensor, bert)\n",
    "            embedding_map[file.split('.')[0]] = embeddings\n",
    "    for file in deceptive:\n",
    "        with open(DECEPTIVE_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            tokenized_text, tokens_tensor, segments_tensor = prepare_text(text, tokenizer)\n",
    "            embeddings = get_bert_embeddings(tokens_tensor, segments_tensor, bert)\n",
    "            embedding_map[file.split('.')[0]] = embeddings\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(embedding_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "bert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "save_embeddings(bert, tokenizer, '../../embeddings/transcript_features_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emotion = AutoModelForSequenceClassification.from_pretrained(\"ncduy/bert-base-cased-finetuned-emotion\", output_hidden_states = True).bert\n",
    "bert_emotion.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ncduy/bert-base-cased-finetuned-emotion\")\n",
    "save_embeddings(bert_emotion, tokenizer, '../../embeddings/transcript_features_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transformersbook/distilbert-base-uncased-finetuned-emotion were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "distil_bert_emotion = AutoModel.from_pretrained(\"transformersbook/distilbert-base-uncased-finetuned-emotion\", output_hidden_states = True)\n",
    "distil_bert_emotion.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"transformersbook/distilbert-base-uncased-finetuned-emotion\")\n",
    "save_embeddings(distil_bert_emotion, tokenizer, '../../embeddings/transcript_features_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 121kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [00:23<00:00, 11.5MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 222kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "distil_bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "distil_bert_model.eval()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "save_embeddings(distil_bert_model, tokenizer, '../../embeddings/transcript_features_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentence_transformer_embeddings(model, output_file):\n",
    "    embedding_map = {}\n",
    "    for file in truthful:\n",
    "        with open(TRUTHFUL_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            embeddings = model.encode(text)\n",
    "            embedding_map[file.split('.')[0]] = embeddings\n",
    "    for file in deceptive:\n",
    "        with open(DECEPTIVE_DIR + file, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            embeddings = model.encode(text)\n",
    "            embedding_map[file.split('.')[0]] = embeddings\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(embedding_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_sentence_transformer_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m miniLM_model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m save_sentence_transformer_embeddings(miniLM_model, \u001b[39m'\u001b[39m\u001b[39m../../embeddings/transcript_features_5.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_sentence_transformer_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "miniLM_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "save_sentence_transformer_embeddings(miniLM_model, '../../embeddings/transcript_features_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_sentence_transformer_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mpnet_model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mall-mpnet-base-v2\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m save_sentence_transformer_embeddings(mpnet_model, \u001b[39m'\u001b[39m\u001b[39m../../embeddings/transcript_features_6.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_sentence_transformer_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "mpnet_model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "save_sentence_transformer_embeddings(mpnet_model, '../../embeddings/transcript_features_6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 47.5kB/s]\n",
      "c:\\Users\\alame\\OneDrive\\Documents\\USC\\CSCI535\\Multimodal Deception Detection\\project-env\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alame\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [02:01<00:00, 11.0MB/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 863kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.00kB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 52.4M/438M [42:26:38<312:09:01, 343B/s]\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-large-uncased', output_hidden_states = True)\n",
    "bert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "save_embeddings(bert, tokenizer, '../../embeddings/transcript_features_7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalBertClassifier(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(LexicalBertClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 64)\n",
    "        self.linear2 = torch.nn.Linear(64, 2)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(torch.nn.functional.relu(x))\n",
    "        return self.linear2(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalContrastiveClassifier(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(LexicalContrastiveClassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 256)\n",
    "        self.linear2 = torch.nn.Linear(256, 64)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(torch.nn.functional.relu(x))\n",
    "        return self.linear2(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalBertClassifierPCA(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(LexicalBertClassifierPCA, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 16)\n",
    "        self.linear2 = torch.nn.Linear(16, 2)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(torch.nn.functional.relu(x))\n",
    "        return self.linear2(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalContrastiveClassifierPCA(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(LexicalContrastiveClassifierPCA, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_size, 32)\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear1(torch.nn.functional.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalBertDataset(Dataset):\n",
    "    def __init__(self, data, embedding_map_file) -> None:\n",
    "        self.data = data\n",
    "        with open(embedding_map_file, 'rb') as f:\n",
    "            self.embedding_map = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embeddings = self.embedding_map[self.data[index][0].split('.')[0]]\n",
    "        return embeddings, self.data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalBertDatasetPCA(Dataset):\n",
    "    def __init__(self, data, embedding_map_file, pca, sc, test) -> None:\n",
    "        self.embeddings = []\n",
    "        self.labels = []\n",
    "        with open(embedding_map_file, 'rb') as f:\n",
    "            self.embedding_map = pickle.load(f)\n",
    "        for file, label in data:\n",
    "            embedding = self.embedding_map[file.split('.')[0]]\n",
    "            if type(embedding) == np.ndarray:\n",
    "                self.embeddings.append(embedding)\n",
    "            else:\n",
    "                self.embeddings.append(embedding.numpy())\n",
    "            self.labels.append(label)\n",
    "        self.embeddings = np.array(self.embeddings)\n",
    "        if not test:\n",
    "            self.embeddings = sc.fit_transform(self.embeddings)\n",
    "            self.embeddings = pca.fit_transform(self.embeddings)\n",
    "        else:\n",
    "            self.embeddings = sc.transform(self.embeddings)\n",
    "            self.embeddings = pca.transform(self.embeddings)\n",
    "        self.embeddings = torch.from_numpy(self.embeddings).float()\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.embeddings[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast(out, y, truth_out, deception_out, margin):\n",
    "    truth_similarity = torch.nn.functional.cosine_similarity(out, truth_out, dim=1)\n",
    "    deception_similarity = torch.nn.functional.cosine_similarity(out, deception_out, dim=1)\n",
    "    loss = torch.sum(y * torch.nn.functional.relu(margin - truth_similarity + deception_similarity) + (1 - y) * torch.nn.functional.relu(margin - deception_similarity + truth_similarity))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(model, train_loader, optimizer, num_epochs, truth_statement, deception_statement, margin):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            truth_out = model(truth_statement)\n",
    "            deception_out = model(deception_statement)\n",
    "            loss = contrast(out, y, truth_out, deception_out, margin)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_contrastive(model, val_loader, truth_statement, deception_statement):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            truth_out = model(truth_statement)\n",
    "            deception_out = model(deception_statement)\n",
    "            truth_similarity = torch.nn.functional.cosine_similarity(out, truth_out, dim=1)\n",
    "            deception_similarity = torch.nn.functional.cosine_similarity(out, deception_out, dim=1)\n",
    "            _, predicted = torch.max(torch.stack((truth_similarity, deception_similarity), dim=1).data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(embeddings_map_file, train_batch_size, do_pca, embedding_size):\n",
    "    truthful_data = [(x, 0) for x in truthful]\n",
    "    deceptive_data = [(x, 1) for x in deceptive]\n",
    "    for i in range(10):\n",
    "        val_data = truthful_data[i*6:(i+1)*6] + deceptive_data[i*6:(i+1)*6]\n",
    "        train_data = truthful_data[:i*6] + truthful_data[(i+1)*6:] + deceptive_data[:i*6] + deceptive_data[(i+1)*6:]\n",
    "        if not do_pca:\n",
    "            train_dataset = LexicalBertDataset(train_data, embeddings_map_file)\n",
    "            val_dataset = LexicalBertDataset(val_data, embeddings_map_file)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=12, shuffle=True)\n",
    "            yield train_loader, val_loader, None\n",
    "        else:\n",
    "            sc = StandardScaler()\n",
    "            pca = PCA(n_components=embedding_size)\n",
    "            train_dataset = LexicalBertDatasetPCA(train_data, embeddings_map_file, pca, sc, False)\n",
    "            val_dataset = LexicalBertDatasetPCA(val_data, embeddings_map_file, pca, sc, True)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=12, shuffle=True)\n",
    "            yield train_loader, val_loader, (pca, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_fold(train_loader, val_loader, num_epochs, embedding_size, contrastive, truth_statement, deception_statement, margin, do_pca):\n",
    "    if not contrastive:\n",
    "        if not do_pca:\n",
    "            model = LexicalBertClassifier(embedding_size)\n",
    "        else:\n",
    "            model = LexicalBertClassifierPCA(embedding_size)\n",
    "        model.to(device)\n",
    "        learning_rate = 1e-3\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "        return eval(model, val_loader)\n",
    "    if contrastive:\n",
    "        if not do_pca:\n",
    "            model = LexicalContrastiveClassifier(embedding_size)\n",
    "        else:\n",
    "            model = LexicalContrastiveClassifierPCA(embedding_size)\n",
    "        model.to(device)\n",
    "        learning_rate = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        train_contrastive(model, train_loader, optimizer, num_epochs, truth_statement, deception_statement, margin)\n",
    "        return eval_contrastive(model, val_loader, truth_statement, deception_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(embeddings_map_file, train_batch_size, num_epochs, embedding_size = 768, contrastive = False, truth_statement = None, deception_statement = None, margin = 0, do_pca = False):\n",
    "    accuracies = []\n",
    "    for train_loader, val_loader, args in kfold(embeddings_map_file, train_batch_size, do_pca, embedding_size):\n",
    "        if do_pca and contrastive:\n",
    "            pca, sc = args\n",
    "            truth_statement_copy = torch.from_numpy(pca.transform(sc.transform([truth_statement.cpu().numpy()]))[0]).float().to(device)\n",
    "            deception_statement_copy = torch.from_numpy(pca.transform(sc.transform([deception_statement.cpu().numpy()]))[0]).float().to(device)\n",
    "        else:\n",
    "            truth_statement_copy = truth_statement\n",
    "            deception_statement_copy = deception_statement\n",
    "        accuracies.append(run_one_fold(train_loader, val_loader, num_epochs, embedding_size, contrastive, truth_statement_copy, deception_statement_copy, margin, do_pca))\n",
    "    print(accuracies)\n",
    "    return sum(accuracies) / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6666666666666666, 0.6666666666666666, 0.5, 0.5833333333333334, 0.6666666666666666, 0.3333333333333333, 0.5833333333333334, 0.75, 0.6666666666666666, 0.6666666666666666]\n",
      "0.6083333333333334\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-base-uncased\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_1.pkl', train_batch_size = 4, num_epochs = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6666666666666666, 0.5833333333333334, 0.25, 0.8333333333333334, 0.6666666666666666, 0.5, 0.6666666666666666, 0.75, 0.75, 0.5833333333333334]\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-base-cased-finetuned-emotion\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_2.pkl', train_batch_size = 4, num_epochs = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.5833333333333334, 0.75, 0.75, 0.75, 0.5, 0.5833333333333334, 0.75, 0.3333333333333333]\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for transformersbook/distilbert-base-uncased-finetuned-emotion\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_3.pkl', train_batch_size = 4, num_epochs = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.5, 0.3333333333333333, 0.5833333333333334, 0.5833333333333334, 0.5, 0.5833333333333334, 0.75, 0.75, 0.4166666666666667]\n",
      "0.5333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for distil-bert-base-uncased\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_4.pkl', train_batch_size = 4, num_epochs = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.25, 0.75, 0.5833333333333334, 0.5833333333333334, 0.3333333333333333, 0.6666666666666666, 0.5833333333333334, 0.5]\n",
      "0.525\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for sentence-transformer/all-MiniLM-L6-v2\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_5.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5833333333333334, 0.5, 0.25, 0.8333333333333334, 0.5833333333333334, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.4166666666666667]\n",
      "0.5666666666666668\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for sentence-transformer/all-mpnet-base-v2\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_6.pkl', train_batch_size = 4, num_epochs = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5206611570247934\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity check for miniLM\n",
    "with open('../../embeddings/transcript_features_5.pkl', 'rb') as f:\n",
    "    embedding_map = pickle.load(f)\n",
    "truth_statement = miniLM_model.encode('This is the truth')\n",
    "deception_statement = miniLM_model.encode('This is a lie')\n",
    "count = 0\n",
    "for i in truthful:\n",
    "    if cosine_similarity(embedding_map[i.split('.')[0]], truth_statement) > cosine_similarity(embedding_map[i.split('.')[0]], deception_statement):\n",
    "        count += 1\n",
    "for i in deceptive:\n",
    "    if cosine_similarity(embedding_map[i.split('.')[0]], deception_statement) > cosine_similarity(embedding_map[i.split('.')[0]], truth_statement):\n",
    "        count += 1\n",
    "print(count / (len(truthful) + len(deceptive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5289256198347108\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity check for mpNet\n",
    "with open('../../embeddings/transcript_features_6.pkl', 'rb') as f:\n",
    "    embedding_map = pickle.load(f)\n",
    "truth_statement = mpnet_model.encode('This is the truth')\n",
    "deception_statement = mpnet_model.encode('This is a lie')\n",
    "count = 0\n",
    "for i in truthful:\n",
    "    if cosine_similarity(embedding_map[i.split('.')[0]], truth_statement) > cosine_similarity(embedding_map[i.split('.')[0]], deception_statement):\n",
    "        count += 1\n",
    "for i in deceptive:\n",
    "    if cosine_similarity(embedding_map[i.split('.')[0]], deception_statement) > cosine_similarity(embedding_map[i.split('.')[0]], truth_statement):\n",
    "        count += 1\n",
    "print(count / (len(truthful) + len(deceptive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.75, 0.5833333333333334, 0.3333333333333333, 0.5, 0.3333333333333333, 0.5, 0.25, 0.4166666666666667, 0.5]\n",
      "0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Contrastive model accuracy for miniLM\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_5.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 384, contrastive = True, truth_statement = torch.tensor(miniLM_model.encode('This is the truth')).to(device), deception_statement = torch.tensor(miniLM_model.encode('This is a lie')).to(device), margin = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4166666666666667, 0.4166666666666667, 0.6666666666666666, 0.25, 0.4166666666666667, 0.3333333333333333, 0.3333333333333333, 0.5, 0.4166666666666667, 0.75]\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "# Contrasting model accuracy for mpNet\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_6.pkl', train_batch_size = 4, num_epochs = 40, contrastive = True, truth_statement = torch.tensor(mpnet_model.encode('This is the truth')).to(device), deception_statement = torch.tensor(mpnet_model.encode('This is a lie')).to(device), margin = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5833333333333334, 0.5, 0.6666666666666666, 0.5, 0.4166666666666667, 0.5, 0.5, 0.6666666666666666, 0.5833333333333334]\n",
      "0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-large-uncased\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_7.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5833333333333334, 0.75, 0.4166666666666667, 0.6666666666666666, 0.6666666666666666, 0.4166666666666667, 0.5833333333333334, 0.6666666666666666, 0.75, 0.5833333333333334]\n",
      "0.6083333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-base-uncased with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_1.pkl', train_batch_size = 2, num_epochs = 20, embedding_size = 64, do_pca = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5833333333333334, 0.4166666666666667, 0.5, 0.6666666666666666, 0.75, 0.4166666666666667, 0.5833333333333334, 0.5833333333333334, 0.4166666666666667, 0.5833333333333334]\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-base-cased-finetuned-emotion with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_2.pkl', train_batch_size = 2, num_epochs = 20, embedding_size = 64, do_pca = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5833333333333334, 0.5, 0.5833333333333334, 0.6666666666666666, 0.5, 0.5833333333333334, 0.5, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334]\n",
      "0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for transformersbook/distilbert-base-uncased-finetuned-emotion with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_3.pkl', train_batch_size = 2, num_epochs = 20, embedding_size=64, do_pca = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.9166666666666666, 0.25, 0.4166666666666667, 0.5, 0.5, 0.75, 0.75, 0.5833333333333334, 0.8333333333333334]\n",
      "0.5999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for distil-bert-base-uncased with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_4.pkl', train_batch_size = 2, num_epochs = 20, embedding_size=100, do_pca = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.5, 0.5833333333333334, 0.6666666666666666, 0.5, 0.5, 0.5, 0.5, 0.75, 0.4166666666666667]\n",
      "0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for sentence-transformer/all-MiniLM-L6-v2 with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_5.pkl', train_batch_size = 2, num_epochs = 20, embedding_size = 100, do_pca=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.8333333333333334, 0.6666666666666666, 0.75, 0.4166666666666667, 0.75, 0.5, 0.5]\n",
      "0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for sentence-transformer/all-mpnet-base-v2 with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_6.pkl', train_batch_size = 2, num_epochs = 20, embedding_size=100, do_pca = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.6666666666666666, 0.5833333333333334, 0.4166666666666667, 0.4166666666666667, 0.4166666666666667, 0.5, 0.3333333333333333, 0.25, 0.25]\n",
      "0.4333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Contrastive model accuracy for miniLM with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_5.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 64, contrastive = True, truth_statement = torch.tensor(miniLM_model.encode('This is the truth')).to(device), deception_statement = torch.tensor(miniLM_model.encode('This is a lie')).to(device), margin = 0.05, do_pca=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.8333333333333334, 0.5, 0.25, 0.5833333333333334, 0.5, 0.5, 0.3333333333333333, 0.16666666666666666, 0.25]\n",
      "0.4416666666666667\n"
     ]
    }
   ],
   "source": [
    "# Contrasting model accuracy for mpNet with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_6.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 64, contrastive = True, truth_statement = torch.tensor(mpnet_model.encode('This is the truth')).to(device), deception_statement = torch.tensor(mpnet_model.encode('This is a lie')).to(device), margin = 0.05, do_pca=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.8333333333333334, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.4166666666666667, 0.5, 0.5833333333333334, 0.3333333333333333, 0.5]\n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy for bert-large-uncased with pca\n",
    "print(get_model_accuracy('../../embeddings/transcript_features_7.pkl', train_batch_size = 4, num_epochs = 40, embedding_size = 64, do_pca = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c59c81481eecbc2634d3b1b1b767a18d06ce834aa17614e768319cde30fd3a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
